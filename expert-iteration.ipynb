{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import threading\n",
    "import queue\n",
    "import os\n",
    "from typing import TypeVar, Dict, Callable, Tuple, Union, List, Generic\n",
    "Fn = Callable\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip(list_of_tuples):\n",
    "    unzipped_as_tuples = list(zip(*list_of_tuples))\n",
    "    return tuple([list(tup) for tup in unzipped_as_tuples])\n",
    "\n",
    "def sample(it):\n",
    "    rand = np.random.sample()\n",
    "    t = 0.0\n",
    "    if isinstance(it, dict):\n",
    "        it = it.items()\n",
    "    else:\n",
    "        it = enumerate(it)\n",
    "    for i, f in it:\n",
    "        t = t + f\n",
    "        if rand <= t:\n",
    "            return i\n",
    "\n",
    "def softmax(x):\n",
    "    if len(x.shape) == 1:\n",
    "        exped = np.exp(x - np.max(x))\n",
    "        return exped / exped.sum()\n",
    "    else:\n",
    "        mx = np.max(x, axis=1)\n",
    "        mx = mx.reshape((1,) + mx.shape).transpose()\n",
    "        exped = np.exp(x - mx)\n",
    "        denom = np.sum(exped, axis=1)\n",
    "        denom = denom.reshape((1,) + denom.shape).transpose()\n",
    "        return exped / denom\n",
    "    \n",
    "def rescale(x):\n",
    "    return x / np.sum(x, axis=x.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "State = TypeVar('State')\n",
    "Action = int\n",
    "Player = int\n",
    "\n",
    "class Game(Generic[State]):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def gen_roots(self, num: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        states, actions = unzip([self.gen_root() for _ in range(num)])\n",
    "        return np.array(states), np.array(actions)\n",
    "    def gen_root(self) -> Tuple[State, Player]:\n",
    "        states, actions = self.gen_roots(1)\n",
    "        return states[0], actions[0]\n",
    "    \n",
    "    def do_actions(self, \n",
    "                   states: np.ndarray, \n",
    "                   actions: np.ndarray, \n",
    "                   players: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        lst = list(map(lambda i: self.do_action(states[i], actions[i], players[i], range(players.size))))\n",
    "        new_states, new_players = unzip(lst)\n",
    "        return np.array(new_states), np.array(new_players)\n",
    "    def do_action(self, state: State, action: Action, player: Player) -> Tuple[State, Player]:\n",
    "        states, players = self.do_actions(np.array([state]), np.array([action]), np.array([player]))\n",
    "        return states[0], players[0]\n",
    "    \n",
    "    def valid_actionses(self, states: np.ndarray, players: np.ndarray) -> np.ndarray:\n",
    "        return np.array(list(map(lambda i: self.valid_actions(states[i], player[i]), range(players.size))))\n",
    "    def valid_actions(self, state: State, player: Player) -> np.ndarray:\n",
    "        return self.valid_actionses(np.array([state]), np.array([player]))[0]\n",
    "    \n",
    "    def check_ends(self, states: np.ndarray, players: np.ndarray) -> np.ndarray:\n",
    "        return np.array(list(map(lambda i: self.check_end(states[i], players[i]), players.size)))\n",
    "    def check_end(self, state: State, player: Player) -> np.ndarray:\n",
    "        return self.check_ends(np.array([state]), np.array([player]))[0]\n",
    "        \n",
    "    @property\n",
    "    def num_actions(self) -> int:\n",
    "        raise NotImplemented\n",
    "    @property\n",
    "    def num_players(self) -> int:\n",
    "        raise NotImplemented\n",
    "        \n",
    "    def render(self, state: State, player: Player):\n",
    "        print('Player %d\\'s turn:' % player)\n",
    "        print(state)\n",
    "        \n",
    "    def parse(self, s: str):\n",
    "        return int(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GamePlayer(Generic[State]):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def take_turn(self, state: State, player: Player, prev_acts: List[Action]) -> Action:\n",
    "        return self.take_turns([state], [player], [prev_acts])[0]\n",
    "        \n",
    "    def take_turns(self, states: np.ndarray, players: np.ndarray, prev_acts: np.ndarray) -> np.ndarray:\n",
    "        return np.array([self.take_turn(states[0], players[0], prev_acts[0])])\n",
    "        \n",
    "def play_game(game: Game[State], mk_players: List[Tuple[List[Player], Fn[[], GamePlayer[State]]]]):\n",
    "    players_for, mk_players = unzip(mk_players)\n",
    "    player_objs = [mk_player() for mk_player in mk_players]\n",
    "    players = { p: player for ps, player in zip(players_for, player_objs) for p in ps }\n",
    "    cur_state, cur_player = game.gen_root()\n",
    "    #state_hist, player_hist = [cur_state], [cur_player]\n",
    "    action_hist = []\n",
    "    cur_ends = game.check_end(cur_state, cur_player)\n",
    "    while not np.any(cur_ends):\n",
    "        cur_action = players[cur_player].take_turn(cur_state, cur_player, action_hist)\n",
    "        action_hist.append(cur_action)\n",
    "        cur_state, cur_player = game.do_action(cur_state, cur_action, cur_player)\n",
    "        #state_hist.append(cur_state)\n",
    "        #player_hist.append(cur_player)\n",
    "        cur_ends = game.check_end(cur_state, cur_player)\n",
    "    return cur_state, cur_ends, player_objs\n",
    "\n",
    "def play_games(num_games: int, game: Game[State], mk_players: List[Tuple[List[Player], Fn[[], GamePlayer[State]]]]):\n",
    "    players_for, mk_players = unzip(mk_players)\n",
    "    player_objs = [mk_player() for mk_player in mk_players]\n",
    "    players = { p: player for ps, player in zip(players_for, player_objs) for p in ps }\n",
    "    cur_states, cur_players = game.gen_roots(num_games)\n",
    "    action_hist = np.empty(num_games, dtype=np.object)\n",
    "    action_hist[:] = [[] for _ in range(num_games)]\n",
    "    ends = np.empty((num_games, game.num_players))\n",
    "    fin_states = np.empty((num_games,) + cur_states.shape[1:], dtype=cur_states.dtype)\n",
    "    idxs = np.arange(num_games)\n",
    "    while cur_players.size:\n",
    "        cur_actions = np.empty(cur_players.size)\n",
    "        for pnums, player in zip(players_for, player_objs):\n",
    "            pidxs = np.concatenate([np.flatnonzero(cur_players - pnum) for pnum in pnums])\n",
    "            actions = player.take_turns(cur_states[pidxs], cur_players[pidxs], action_hist[pidxs])\n",
    "            cur_actions[pidxs] = actions\n",
    "        for i, act in enumerate(cur_actions):\n",
    "            action_hist[i].append(act)\n",
    "        cur_states, cur_players = game.do_actions(cur_states, actions, cur_players)\n",
    "        cur_ends = game.check_ends(cur_states, cur_players)\n",
    "        mask = np.any(cur_ends, axis=1)\n",
    "        ends[idxs[mask]] = cur_ends[mask]\n",
    "        fin_states[idxs[mask]] = cur_states[mask]\n",
    "        cur_states = cur_states[~mask]\n",
    "        cur_players = cur_players[~mask]\n",
    "        action_hist = action_hist[~mask]\n",
    "        idxs = idxs[~mask]\n",
    "    return fin_states, ends, player_objs\n",
    "\n",
    "class UserPlayer(GamePlayer[State]):\n",
    "    def __init__(self, game: Game[State]):\n",
    "        self.game = game\n",
    "    \n",
    "    def take_turn(self, state: State, player: Player, _):\n",
    "        self.game.render(state, player)\n",
    "        ret = input('What is your move? ')\n",
    "        while True:\n",
    "            try:\n",
    "                ret = self.game.parse(ret)\n",
    "            except ValueError:\n",
    "                ret = input('What is your move? ')\n",
    "            else:\n",
    "                return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator(Generic[State]):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def eval_states(states: np.ndarray, players: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        pass\n",
    "    \n",
    "    def eval_state(state: State, player: Player) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        pass\n",
    "    \n",
    "class EvaluatorFromFn(Evaluator[State]):\n",
    "    def __init__(self, fn, plural=False):\n",
    "        if plural:\n",
    "            self.eval_states = fn\n",
    "        else:\n",
    "            self.eval_state = fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_puct = 1.0\n",
    "dirichlet_eps = 0.25\n",
    "dirichlet_alpha = 0.3\n",
    "\n",
    "class MCTSNode(Generic[State]):\n",
    "    def __init__(self, state: State):\n",
    "        self.state = state\n",
    "        \n",
    "    @property\n",
    "    def terminal(self):\n",
    "        raise NotImplemented\n",
    "\n",
    "class InternalMCTSNode(MCTSNode[State]):\n",
    "    terminal = False\n",
    "    \n",
    "    def __init__(self, state: State, probs: np.ndarray, player: Player) -> None:\n",
    "        super().__init__(state)\n",
    "        self.player = player\n",
    "        \n",
    "        self.actions = np.flatnonzero(probs)\n",
    "        self.inv_actions = np.empty_like(probs, dtype=np.int)\n",
    "        self.inv_actions[self.actions] = np.arange(self.actions.size)\n",
    "        \n",
    "        self.N: np.ndarray = np.zeros_like(self.actions, dtype=np.int)\n",
    "        self.W: np.ndarray = np.zeros_like(self.actions, dtype=np.float)\n",
    "        self.Q: np.ndarray = np.zeros_like(self.actions, dtype=np.float)\n",
    "        self.P: np.ndarray = probs[self.actions]\n",
    "        \n",
    "        self.total_visits: int = 1\n",
    "        self.children: List[MCTSNode] = [None] * probs.size\n",
    "            \n",
    "    def select_action(self) -> Action:\n",
    "        if not self.actions.size:\n",
    "            return None\n",
    "        \n",
    "        root_total = math.sqrt(self.total_visits)\n",
    "        \n",
    "        selection_priorities = self.Q + c_puct * root_total * self.P / (1 + self.N)\n",
    "        return self.actions[np.argmax(selection_priorities)]\n",
    "    \n",
    "    def backup(self, a, vs):\n",
    "        self.total_visits = self.total_visits + 1\n",
    "        idx = self.inv_actions[a]\n",
    "        self.N[idx] = self.N[idx] + 1\n",
    "        self.W[idx] = self.W[idx] + vs[self.player]\n",
    "        self.Q[idx] = self.W[idx] / self.N[idx]\n",
    "    \n",
    "    def probs(self, temp) -> np.ndarray:\n",
    "        if temp == 0.0:\n",
    "            ret = np.zeros_like(self.inv_actions)\n",
    "            ret[self.actions[np.argmax(self.N)]] = 1.0\n",
    "            return ret\n",
    "        exps = self.N ** (1.0 / temp)\n",
    "        ret = np.zeros_like(self.inv_actions, dtype=np.float)\n",
    "        ret[self.actions] = exps / np.sum(exps)\n",
    "        return ret\n",
    "    \n",
    "class LeafMCTSNode(MCTSNode[State]):\n",
    "    terminal = True\n",
    "    \n",
    "    def __init__(self, state: State, values: np.ndarray):\n",
    "        super().__init__(state)\n",
    "        self.values = values\n",
    "\n",
    "class MCTSPlayer(GamePlayer[State]):\n",
    "    def __init__(self,\n",
    "                 game: Game[State],\n",
    "                 evaluator: Evaluator[State],\n",
    "                 search_size: int,\n",
    "                 temp: float = 0.0):\n",
    "        self.game = game\n",
    "        self.evaluator = evaluator\n",
    "        self.search_size = search_size\n",
    "        self.node: MCTSNode[State]\n",
    "        self.hist: List[Tuple[State, np.ndarray, Player]]\n",
    "        self.prev_turn = -1\n",
    "        self.temp = temp\n",
    "\n",
    "    def take_turn(self, state: State, player: Player, prev_actions: np.ndarray) -> np.ndarray:\n",
    "        cur_turn = len(prev_actions)\n",
    "        \n",
    "        if self.prev_turn < 0 or cur_turn <= self.prev_turn:\n",
    "            probs, _ = self.evaluator.eval_state(state, player)\n",
    "            mask = probs != 0\n",
    "            probs[mask] = ((1 - dirichlet_eps) * probs[mask] + \n",
    "                     dirichlet_eps * np.random.dirichlet(np.full_like(probs[mask], dirichlet_alpha)))\n",
    "            self.node = InternalMCTSNode(state, probs, player)\n",
    "            self.hist = []\n",
    "        else:\n",
    "            while self.prev_turn < cur_turn:\n",
    "                self.node = self.node.children[prev_actions[self.prev_turn]]\n",
    "                if not self.node:\n",
    "                    probs, _ = self.evaluator.eval_state(state, player)\n",
    "                    self.node = InternalMCTSNode(state, probs, player)\n",
    "                    break\n",
    "                self.prev_turn += 1\n",
    "                \n",
    "        self.prev_turn = cur_turn\n",
    "        \n",
    "        return self._take_turn()\n",
    "    \n",
    "    #def take_turns(self, states: np.ndarray, players: np.ndarray, prev_actions: List[Action]) -> Action:\n",
    "    #    cur_turn = len(prev_actions)\n",
    "    #    \n",
    "    #    if self.prev_turn < 0 or cur_turn <= self.prev_turn:\n",
    "    #        probs, _ = self.evaluator.eval_states(states, players)\n",
    "    #        mask = probs == 0\n",
    "    #        dirichlet = np.random.dirichlet(np.full(self.game.num_actions), (players.size,))\n",
    "    #        dirichlet[mask] = 0\n",
    "    #        dirichlet = rescale(dirichlet)\n",
    "    #        probs = (1 - dirichlet_eps) * probs + dirichlet_eps * dirichlet\n",
    "            \n",
    "        \n",
    "    def _take_turn(self):\n",
    "        for _ in range(self.search_size):\n",
    "            cur_node = self.node\n",
    "            history: List[Tuple[MCTSNode, Action]] = []\n",
    "\n",
    "            while cur_node and not cur_node.terminal:\n",
    "                action = cur_node.select_action()\n",
    "                history.append((cur_node, action))\n",
    "                cur_node = cur_node.children[action]\n",
    "\n",
    "            rewards: float\n",
    "            if cur_node:\n",
    "                rewards = cur_node.values\n",
    "            else:\n",
    "                prev_node = history[-1][0]\n",
    "                action = history[-1][1]\n",
    "                new_state, player = self.game.do_action(\n",
    "                    prev_node.state, action, prev_node.player)\n",
    "                result = self.game.check_end(new_state, player)\n",
    "                if np.any(result):\n",
    "                    rewards = rewards_from_result(result)\n",
    "                    prev_node.children[action] = LeafMCTSNode(new_state, rewards)\n",
    "                else:\n",
    "                    probs, rewards = self.evaluator.eval_state(new_state, player)\n",
    "                    prev_node.children[action] = InternalMCTSNode(new_state, probs, player)\n",
    "\n",
    "            for node, act in history:\n",
    "                node.backup(act, rewards)\n",
    "        \n",
    "        probs = self.node.probs(self.temp)\n",
    "        self.hist.append((self.node.state, probs, self.node.player))\n",
    "        return sample(probs)\n",
    "    \n",
    "def rewards_from_result(game_result: np.ndarray):\n",
    "    winners = np.count_nonzero(game_result)\n",
    "    if winners == game_result.size:\n",
    "        return np.zeros_like(game_result, dtype=np.float)\n",
    "    losers = game_result.size - winners\n",
    "    values = np.full_like(game_result, -1.0 / losers, dtype=np.float)\n",
    "    values[game_result] = 1.0 / winners\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcts_play_self(game: Game[State], evaluator: Evaluator[State], search_size: int, temp: float = 0.0):\n",
    "    end_state, result, players = play_games(1, game, [([0, 1], lambda: MCTSPlayer(game, evaluator, search_size, temp))])\n",
    "    hist = players[0][0].hist\n",
    "    rewards = rewards_from_result(result[0])\n",
    "    \n",
    "    return [(state, probs, rewards, player) for state, probs, player in hist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "TTTState = Tuple[np.ndarray, int]\n",
    "\n",
    "class TTT(Game[TTTState]):\n",
    "    _board_shape = (9,)\n",
    "    num_actions = 9\n",
    "    num_players = 2\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def gen_roots(self, num_roots: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        return np.zeros((num_roots,) + TTT._board_shape, dtype=np.int), np.full(num_roots, 1)\n",
    "\n",
    "    def do_actions(self, states: np.ndarray, actions: np.ndarray, players: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        new_states = np.copy(states)\n",
    "        new_states[np.arange(new_states.shape[0]),actions] = -1 + 2 * players\n",
    "        return (new_states, 1 - players)\n",
    "\n",
    "    def valid_actionses(self, board: np.ndarray, _) -> np.ndarray:\n",
    "        return board == 0\n",
    "\n",
    "    _end_idx = sum([[3 * i, 3 * i + 1, 3 * i + 2, i, i + 3, i + 6 ] for i in range(3)], [])\n",
    "    _end_idx = np.array(_end_idx + [2, 4, 6, 0, 4, 8])\n",
    "    _inv_end_idx = np.array(sum([[3 * i, i] for i in range(3)], []) + [2, 0])\n",
    "    def check_ends(self, boards: np.ndarray, _) -> np.ndarray:\n",
    "        ret = np.zeros((boards.shape[0], 2), dtype=np.bool)\n",
    "        game, pos = np.nonzero(np.abs(np.sum(boards[:,TTT._end_idx].reshape(boards.shape[0], 8, 3), axis=2)) == 3)\n",
    "        ret[game, (boards[game, TTT._inv_end_idx[pos]] + 1) // 2] = True\n",
    "        ret[np.all(boards, axis=1),:] = True\n",
    "        return ret\n",
    "    \n",
    "    ch = {-1: 'o', 0: ' ', 1: 'x'}\n",
    "    def render(self, state: State, player: Player):\n",
    "        for i in range(3):\n",
    "            if i != 0:\n",
    "                print('-----')\n",
    "            for j in range(3):\n",
    "                if j != 0:\n",
    "                    print('|', end='')\n",
    "                print(TTT.ch[state[3 * i + j]], end='')\n",
    "            print('')\n",
    "        print('%s\\' turn' % ('x' if player == 1 else 'o'))\n",
    "        \n",
    "    def parse(self, s: str):\n",
    "        try:\n",
    "            x, y = s.split()\n",
    "            res = 3 * (int(x) - 1) + int(y) - 1\n",
    "        except:\n",
    "            raise ValueError\n",
    "        return res\n",
    "                \n",
    "    \n",
    "ttt = TTT()\n",
    "def _ttt_eval_state(s: TTTState, p: Player) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    acts = np.zeros(TTT._board_shape)\n",
    "    valid_acts = ttt.valid_actions(s, p)\n",
    "    acts[valid_acts] = 1.0 / np.count_nonzero(valid_acts)\n",
    "    return (acts, np.zeros(2))\n",
    "ttt_evaluator = EvaluatorFromFn(_ttt_eval_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not numpy.float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-113-39f881071344>\u001b[0m in \u001b[0;36mmcts_play_self\u001b[1;34m(game, evaluator, search_size, temp)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmcts_play_self\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mGame\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mState\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mEvaluator\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mState\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearch_size\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mend_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplayers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplay_games\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mMCTSPlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mhist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mrewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrewards_from_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-110-41e178912d55>\u001b[0m in \u001b[0;36mplay_games\u001b[1;34m(num_games, game, mk_players)\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mpnums\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplayers_for\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplayer_objs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[0mpidxs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur_players\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mpnum\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpnum\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpnums\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m             \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake_turns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur_states\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpidxs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_players\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpidxs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_hist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpidxs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m             \u001b[0mcur_actions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpidxs\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mact\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-110-41e178912d55>\u001b[0m in \u001b[0;36mtake_turns\u001b[1;34m(self, states, players, prev_acts)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtake_turns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplayers\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprev_acts\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake_turn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprev_acts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mplay_game\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mGame\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mState\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmk_players\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mPlayer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFn\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGamePlayer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mState\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-112-80aca0c7cf34>\u001b[0m in \u001b[0;36mtake_turn\u001b[1;34m(self, state, player, prev_actions)\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprev_turn\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mcur_turn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mprev_actions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprev_turn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m                     \u001b[0mprobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not numpy.float64"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#%lprun -f MCTSPlayer._take_turn \n",
    "res = mcts_play_self(ttt, ttt_evaluator, 10000)\n",
    "for s, _, _, _ in res:\n",
    "    plt.matshow(s.reshape(3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_search_size = 20\n",
    "\n",
    "class Model(Generic[State]):\n",
    "    def __init__(self, game: Game[State]):\n",
    "        self.game = game\n",
    "    \n",
    "    PARAMS_TRAIN = 0\n",
    "    PARAMS_BEST = 1\n",
    "    \n",
    "    def eval_states(self, \n",
    "                    states: List[State],\n",
    "                    players: List[Player],\n",
    "                    using=PARAMS_BEST) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        probs, values = unzip([self.eval_states(states[i], players[i], using) for i in range(len(players))])\n",
    "        return np.array(probs), np.array(values)\n",
    "    \n",
    "    def eval_state(self, state: State, player: Player, using=PARAMS_BEST) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        a, v = self.eval_states([state], [player], using)\n",
    "        return a[0], v[0]\n",
    "    \n",
    "    def train(self, batch_size=64, num_iters=1000):\n",
    "        pass\n",
    "    \n",
    "    def add_data(self, \n",
    "                 states: List[State], \n",
    "                 probs: List[np.ndarray],\n",
    "                 rewards: List[float],\n",
    "                 players: List[int]):\n",
    "        pass\n",
    "\n",
    "class ExpertIterationTrainer(Generic[State]):\n",
    "    def __init__(self,\n",
    "                 model: Model[State],\n",
    "                 num_iterations: int,\n",
    "                 iteration_size: int = 100,\n",
    "                 search_size: int = 100):\n",
    "        self.model = model\n",
    "        self.game = self.model.game\n",
    "        self.num_iterations = num_iterations\n",
    "        self.iteration_size = iteration_size\n",
    "        self.search_size = search_size\n",
    "              \n",
    "    def train_player(self):\n",
    "        evaluator = EvaluatorFromFn(lambda s, p: self.model.eval_state(s, p))\n",
    "        example_games = [mcts_play_self(self.game, evaluator, example_search_size)]\n",
    "        \n",
    "        for i in range(1, self.num_iterations + 1):\n",
    "            example_games.append(mcts_play_self(self.game, evaluator, example_search_size))\n",
    "            new_probs = [self.model.eval_state(res[0], res[3]) for res in example_games[-1]]\n",
    "            example_probs = [new_probs]\n",
    "            \n",
    "            states, probs, rewards, players = self.play_games()\n",
    "                \n",
    "            self.model.add_data(states, probs, rewards, players)\n",
    "            self.model.train()\n",
    "\n",
    "            print('finished step %d' % i)\n",
    "\n",
    "        example_games.append(mcts_play_self(self.game, evaluator, example_search_size))\n",
    "        new_probs = [self.model.eval_state(res[0], res[3]) for res in example_games[-1]]\n",
    "        example_probs.append(new_probs)\n",
    "\n",
    "        return example_games, EvaluatorFromFn(lambda s, p: self.model.eval_state(s, p, using=Model.PARAMS_BEST))\n",
    "    \n",
    "\n",
    "    def play_games(self) -> Tuple[List[State], \n",
    "                                  List[np.ndarray], \n",
    "                                  List[float], \n",
    "                                  List[int]]:\n",
    "        work_q = queue.Queue()\n",
    "        result_q = queue.Queue()\n",
    "        go = queue.Queue()\n",
    "        alive = [self.iteration_size]\n",
    "        in_q = [0]\n",
    "        counter_lock = threading.Lock()\n",
    "        \n",
    "        def mp_play_self():\n",
    "            my_q = queue.Queue()\n",
    "\n",
    "            def eval_state(state: State, p: Player) -> Tuple[np.ndarray, float]:\n",
    "                with counter_lock:\n",
    "                    work_q.put(((state, p), my_q))\n",
    "                    in_q[0] += 1\n",
    "                    if alive[0] == in_q[0]:\n",
    "                        go.put(True)\n",
    "                return my_q.get()\n",
    "\n",
    "            result = mcts_play_self(self.game, EvaluatorFromFn(eval_state), self.search_size, 1.0)\n",
    "            with counter_lock:\n",
    "                alive[0] -= 1\n",
    "                result_q.put(result)\n",
    "                if alive[0] == 0:\n",
    "                    go.put(False)\n",
    "                elif alive[0] == in_q[0]:\n",
    "                    go.put(True)\n",
    "        \n",
    "        for _ in range(self.iteration_size):\n",
    "            threading.Thread(target=mp_play_self).start()\n",
    "\n",
    "        while go.get():\n",
    "            with counter_lock:\n",
    "                ret_qs = []\n",
    "                states = []\n",
    "                players = []\n",
    "                for _ in range(in_q[0]):\n",
    "                    (state, player), ret_q = work_q.get_nowait()\n",
    "                    ret_qs.append(ret_q)\n",
    "                    states.append(state)\n",
    "                    players.append(player)\n",
    "                in_q[0] = 0\n",
    "                if len(states) > 0:\n",
    "                    a, v = self.model.eval_states(states, players, using=Model.PARAMS_BEST)\n",
    "                    for j, ret_q in enumerate(ret_qs):\n",
    "                        ret_q.put((a[j], v[j]))\n",
    "                        \n",
    "        positions = []\n",
    "        while not result_q.empty():\n",
    "            positions += result_q.get_nowait()\n",
    "        return unzip(positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TTTModel(Model[TTTState]):\n",
    "    game = TTT()\n",
    "    \n",
    "    l2_loss_coeff = 0.01\n",
    "    hidden_size = 100\n",
    "    search_size = 100\n",
    "\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        ttt_hw = tf.Variable(tf.truncated_normal([9, hidden_size]))\n",
    "        ttt_hb = tf.Variable(tf.zeros([hidden_size]))\n",
    "    \n",
    "        ttt_aw = tf.Variable(tf.truncated_normal([hidden_size, 9]))\n",
    "        ttt_ab = tf.Variable(tf.zeros([9]))\n",
    "\n",
    "        ttt_vw = tf.Variable(tf.truncated_normal([hidden_size, 2]))\n",
    "        ttt_vb = tf.Variable(tf.zeros([2]))\n",
    "\n",
    "        weights = [ttt_hw, ttt_hb, ttt_aw, ttt_ab, ttt_vw, ttt_vb]\n",
    "        saver = tf.train.Saver(weights)\n",
    "\n",
    "        tf_boards = tf.placeholder(tf.float32, shape=[None, 9])\n",
    "        hidden = tf.nn.relu(tf.matmul(tf_boards, ttt_hw) + ttt_hb)\n",
    "        actions = tf.matmul(hidden, ttt_aw) + ttt_ab\n",
    "        values = tf.tanh(tf.matmul(hidden, ttt_vw) + ttt_vb)\n",
    "\n",
    "        tf_probs = tf.placeholder(tf.float32, shape=[None, 9])\n",
    "        tf_zs = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "\n",
    "        loss = tf.reduce_mean(tf.square(tf_zs - values))\n",
    "        loss = loss + tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(labels=tf_probs, logits=actions))\n",
    "        for w in weights:\n",
    "            loss = loss + l2_loss_coeff * tf.nn.l2_loss(w)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "        \n",
    "    chkpnt_folder = os.path.join('.', 'checkpoints')\n",
    "    if not os.path.exists(chkpnt_folder):\n",
    "        os.makedirs(chkpnt_folder)\n",
    "    chkpnt_file = os.path.join(chkpnt_folder, 'ttt') \n",
    "            \n",
    "       \n",
    "    def __init__(self):\n",
    "        super().__init__(self.game)\n",
    "        \n",
    "        self.states = np.empty((0, 9))\n",
    "        self.probs = np.empty((0, 9))\n",
    "        self.rewards = np.empty((0, 2))\n",
    "        self.players = np.empty((0,), dtype=np.int)\n",
    "        self.cutoffs = []\n",
    "            \n",
    "        self.best_evaluator = EvaluatorFromFn(lambda s, p: self.eval_state(s, p, using=Model.PARAMS_BEST))\n",
    "        self.train_evaluator = EvaluatorFromFn(lambda s, p: self.eval_state(s, p, using=Model.PARAMS_TRAIN))\n",
    "        \n",
    "        self.train_step = 0\n",
    "        self.train_sess = None\n",
    "        self.best_sess = None\n",
    "        self.best_chkpnt = None\n",
    "        \n",
    "    def __enter__(self):\n",
    "        self.train_sess = tf.Session(graph=self.graph)\n",
    "        self.best_sess = tf.Session(graph=self.graph)\n",
    "        self.train_sess.__enter__()\n",
    "        self.best_sess.__enter__()\n",
    "        \n",
    "        self.train_sess.run(tf.global_variables_initializer())\n",
    "        self.best_chkpnt = self.saver.save(self.train_sess, self.chkpnt_file, 0)\n",
    "        self.saver.restore(self.best_sess, self.best_chkpnt)\n",
    "  \n",
    "        return self\n",
    "        \n",
    "    def __exit__(self, tp, val, traceback):\n",
    "        self.best_sess.__exit__(tp, val, traceback)\n",
    "        self.train_sess.__exit__(tp, val, traceback)\n",
    "        \n",
    "    def eval_states(self, \n",
    "                    states: List[TTTState],\n",
    "                    players: List[Player],\n",
    "                    using=Model.PARAMS_BEST) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        feed = { self.tf_boards: np.array(states) }\n",
    "        sess = self.best_sess if using == Model.PARAMS_BEST else self.train_sess\n",
    "        actions, values = sess.run([self.actions, self.values], feed_dict=feed)\n",
    "        for i in range(actions.shape[0]):\n",
    "            action = actions[i]\n",
    "            poss_actions = self.game.valid_actions(states[i], players[i])\n",
    "            action[~poss_actions] = 0.0\n",
    "            action[poss_actions] = softmax(action[poss_actions])\n",
    "\n",
    "        return (actions, values)\n",
    "    \n",
    "    def eval_state(self, state: State, player: Player, using=Model.PARAMS_BEST) -> Tuple[np.ndarray, float]:\n",
    "        a, v = self.eval_states([state], [player], using)\n",
    "        return a[0], v[0]\n",
    "    \n",
    "    def train(self, batch_size=64, num_iters=1000):\n",
    "        self.train_step += 1\n",
    "        \n",
    "        for i in range(1000):\n",
    "            select = np.random.choice(np.arange(len(self.states)), 64)\n",
    "\n",
    "            feed = {\n",
    "                self.tf_boards: self.states[select],\n",
    "                self.tf_probs: self.probs[select],\n",
    "                self.tf_zs: self.rewards[select]\n",
    "            }\n",
    "            \n",
    "            self.train_sess.run(self.optimizer, feed_dict=feed)\n",
    "                \n",
    "        if self.train_is_better():\n",
    "            self.best_chkpnt = self.saver.save(self.train_sess, self.chkpnt_file, self.train_step)\n",
    "            self.saver.restore(self.best_sess, self.best_chkpnt)\n",
    "        else:\n",
    "            self.saver.restore(self.train_sess, self.best_chkpnt)\n",
    "\n",
    "    def train_is_better(self):\n",
    "        tot_games = 10\n",
    "        reward = 0\n",
    "        mk_best = lambda: MCTSPlayer(self.game, self.best_evaluator, self.search_size)\n",
    "        mk_train = lambda: MCTSPlayer(self.game, self.train_evaluator, self.search_size)\n",
    "        for i in range(tot_games):\n",
    "            reward += rewards_from_result(play_game(self.game, [([0], mk_best), ([1], mk_train)])[1])[1]\n",
    "            reward += rewards_from_result(play_game(self.game, [([0], mk_train), ([1], mk_best)])[1])[0]\n",
    "        \n",
    "        avg_reward = reward / (2 * tot_games)\n",
    "        print(avg_reward)\n",
    "        return avg_reward > 0.1\n",
    "            \n",
    "    \n",
    "    def add_data(self, \n",
    "                 states: List[TTTState], \n",
    "                 probs: List[np.ndarray],\n",
    "                 rewards: List[float],\n",
    "                 players: List[int]):\n",
    "        start_i = 0\n",
    "        if len(self.cutoffs) > 3:\n",
    "            start_i = self.cutoffs[0]\n",
    "            self.cutoffs = self.cutoffs[1:]\n",
    "        self.states = np.concatenate([self.states[start_i:], states])\n",
    "        self.probs = np.concatenate([self.probs[start_i:], probs])\n",
    "        self.rewards = np.concatenate([self.rewards[start_i:], rewards])\n",
    "        self.players = np.concatenate([self.players[start_i:], players])\n",
    "        self.cutoffs.append(len(players))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from .\\checkpoints\\ttt-0\n",
      "0.2\n",
      "INFO:tensorflow:Restoring parameters from .\\checkpoints\\ttt-1\n",
      "finished step 1\n",
      "0.0\n",
      "INFO:tensorflow:Restoring parameters from .\\checkpoints\\ttt-1\n",
      "finished step 2\n",
      "0.5\n",
      "INFO:tensorflow:Restoring parameters from .\\checkpoints\\ttt-3\n",
      "finished step 3\n",
      "0.65\n",
      "INFO:tensorflow:Restoring parameters from .\\checkpoints\\ttt-4\n",
      "finished step 4\n",
      "0.25\n",
      "INFO:tensorflow:Restoring parameters from .\\checkpoints\\ttt-5\n",
      "finished step 5\n",
      "play against computer? (y/n)y\n",
      " | | \n",
      "-----\n",
      " | | \n",
      "-----\n",
      " | | \n",
      "x' turn\n",
      "What is your move? 2 2\n",
      " | | \n",
      "-----\n",
      " |x| \n",
      "-----\n",
      "o| | \n",
      "x' turn\n",
      "What is your move? 1 1\n",
      "x| | \n",
      "-----\n",
      " |x| \n",
      "-----\n",
      "o| |o\n",
      "x' turn\n",
      "What is your move? 3 2\n",
      "x|o| \n",
      "-----\n",
      " |x| \n",
      "-----\n",
      "o|x|o\n",
      "x' turn\n",
      "What is your move? 2 1\n",
      "x|o|o\n",
      "-----\n",
      "x|x| \n",
      "-----\n",
      "o|x|o\n",
      "x' turn\n",
      "What is your move? n\n",
      "What is your move? 2 3\n",
      "play against computer? (y/n)n\n",
      "(array([ 0.05951609,  0.29495674,  0.16805883,  0.10343997,  0.10639825,\n",
      "        0.03845243,  0.078618  ,  0.08965635,  0.06090328], dtype=float32), array([-0.42325997,  0.42405766], dtype=float32))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a95bb31128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a95ba6d668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a95be27160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a95d24e4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a95d741dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a95bb95be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a95bab83c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with TTTModel() as model:\n",
    "    trainer = ExpertIterationTrainer(model, 5)\n",
    "    examples, evaluator = trainer.train_player()\n",
    "    while input('play against computer? (y/n)') == 'y':\n",
    "        play_game(ttt, [([0], lambda: MCTSPlayer(ttt, evaluator, 100)), ([1], lambda: UserPlayer(ttt))])\n",
    "    print(evaluator.eval_state(*ttt.gen_root()))\n",
    "\n",
    "for g in examples[::1]:\n",
    "    plt.matshow(g[-1][0].reshape(3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
