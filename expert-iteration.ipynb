{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import threading\n",
    "import queue\n",
    "import os\n",
    "from typing import TypeVar, Dict, Callable, Tuple, Union, List, Generic, Set, NewType\n",
    "Fn = Callable\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip(list_of_tuples):\n",
    "    unzipped_as_tuples = list(zip(*list_of_tuples))\n",
    "    return tuple([list(tup) for tup in unzipped_as_tuples])\n",
    "\n",
    "def sample(it):\n",
    "    rand = np.random.sample()\n",
    "    t = 0.0\n",
    "    if isinstance(it, dict):\n",
    "        it = it.items()\n",
    "    else:\n",
    "        it = enumerate(it)\n",
    "    for i, f in it:\n",
    "        t = t + f\n",
    "        if rand <= t:\n",
    "            return i\n",
    "\n",
    "def softmax(x):\n",
    "    if len(x.shape) == 1:\n",
    "        exped = np.exp(x - np.max(x))\n",
    "        return exped / exped.sum()\n",
    "    else:\n",
    "        mx = np.max(x, axis=1)\n",
    "        mx = mx.reshape((1,) + mx.shape).transpose()\n",
    "        exped = np.exp(x - mx)\n",
    "        denom = np.sum(exped, axis=1)\n",
    "        denom = denom.reshape((1,) + denom.shape).transpose()\n",
    "        return exped / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "State = TypeVar('State')\n",
    "Action = NewType('Action', int)\n",
    "Player = NewType('Player', int)\n",
    "\n",
    "class Game(Generic[State]):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def gen_roots(self, num: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        return np.array([self.gen_root() for _ in range(num)])\n",
    "    def gen_root(self) -> Tuple[State, Player]:\n",
    "        return self.gen_roots(1)[0]\n",
    "    \n",
    "    def do_actions(self, \n",
    "                   states: np.ndarray, \n",
    "                   actions: np.ndarray, \n",
    "                   players: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        lst = list(map(lambda i: self.do_action(states[i], actions[i], players[i], range(players.size))))\n",
    "        new_states, new_players = unzip(lst)\n",
    "        return np.array(new_states), np.array(new_players)\n",
    "    def do_action(self, state: State, action: Action, player: Player) -> Tuple[State, Player]:\n",
    "        states, players = self.do_actions(np.array([state]), np.array([action]), np.array([player]))\n",
    "        return states[0], players[0]\n",
    "    \n",
    "    def valid_actionses(self, states: np.ndarray, players: np.ndarray) -> np.ndarray:\n",
    "        return np.array(list(map(lambda i: self.valid_actions(states[i], player[i]), range(players.size))))\n",
    "    def valid_actions(self, state: State, player: Player) -> np.ndarray:\n",
    "        return valid_actionses(np.array([state]), np.array([player]))[0]\n",
    "    \n",
    "    def check_ends(self, states: np.ndarray, players: np.ndarray) -> np.ndarray:\n",
    "        return np.array(list(map(lambda i: self.check_end(states[i], players[i]), players.size)))\n",
    "    def check_end(self, state: State, player: Player) -> np.ndarray:\n",
    "        return check_ends(np.array([state]), np.array([player]))[0]\n",
    "        \n",
    "    @property\n",
    "    def num_actions(self) -> int:\n",
    "        raise NotImplemented\n",
    "    @property\n",
    "    def num_players(self) -> int:\n",
    "        raise NotImplemented\n",
    "        \n",
    "    def render(self, state: State, player: Player):\n",
    "        print('Player %d\\'s turn:' % player)\n",
    "        print(state)\n",
    "        \n",
    "    def parse(self, s: str):\n",
    "        return int(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GamePlayer(Generic[State]):\n",
    "    \"\"\"\n",
    "    Plays games.\n",
    "    \n",
    "    Since some GamePlayers need to maintain internal state, it is expected that next_turn\n",
    "    is called exactly once, in order, for each turn of the game, regardless of whether it's\n",
    "    the current player's turn.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, players: Set[Player], take_turn = None):\n",
    "        \"\"\"\n",
    "        Which players this Player object is playing as.\n",
    "        \"\"\"\n",
    "        self.players = players\n",
    "        if take_turn:\n",
    "            self._take_turn = take_turn\n",
    "    \n",
    "    def next_turn(self, state: State, player: Player, prev_act: Action) -> Action:\n",
    "        \"\"\"\n",
    "        Should be called every turn, regardless of whether it is the player's turn.\n",
    "        Returns the player's move if it's their turn, otherwise returns None.\n",
    "        \"\"\"\n",
    "        if player in self.players:\n",
    "            return self._take_turn(state, player, prev_act)\n",
    "        else:\n",
    "            self._watch_turn(state, player, prev_act)\n",
    "            return None\n",
    "    \n",
    "    def _take_turn(self, state: State, player: Player, prev_act: Action) -> Action:\n",
    "        \"\"\"\n",
    "        Select the best move.\n",
    "        \"\"\"\n",
    "        raise NotImplemented\n",
    "    \n",
    "    def _watch_turn(self, state: State, player: Player, prev_act: Action):\n",
    "        \"\"\"\n",
    "        Keep the player's internal state up to date with the game state.\n",
    "        Unnecessary if the player has no internal state.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "class GameAlgorithm(Generic[State]):\n",
    "    \"\"\"\n",
    "    Instantiates GamePlayers with a strategy.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, game: Game[State]):\n",
    "        self.game = game\n",
    "        \n",
    "    def mk_player(self, players: Set[Player]) -> GamePlayer[State]:\n",
    "        raise NotImplemented\n",
    "        \n",
    "    def __call__(self, players: Set[Player]) -> GamePlayer[State]:\n",
    "        \"\"\"\n",
    "        Simply calls mk_player, provided for convenience.\n",
    "        \"\"\"\n",
    "        return self.mk_player(players)\n",
    "\n",
    "def play_game(game: Game[State], mk_players: List[Tuple[Set[Player], GameAlgorithm[State]]]):\n",
    "    player_objs = [ alg.mk_player(players) for players, alg in mk_players ]\n",
    "    players_for, _ = unzip(mk_players)\n",
    "    players = { p: player for ps, player in zip(players_for, player_objs) for p in ps }\n",
    "    cur_action = None\n",
    "    cur_state, cur_player = game.gen_root()\n",
    "    cur_ends = game.check_end(cur_state, cur_player)\n",
    "    while not np.any(cur_ends):\n",
    "        cur_action = players[cur_player].next_turn(cur_state, cur_player, cur_action)\n",
    "        cur_state, cur_player = game.do_action(cur_state, cur_action, cur_player)\n",
    "        cur_ends = game.check_end(cur_state, cur_player)\n",
    "    return cur_state, cur_ends, player_objs\n",
    "\n",
    "class UserAlgorithm(GameAlgorithm[State]):\n",
    "    def __init__(self, game: Game[State]):\n",
    "        self.game = game\n",
    "        \n",
    "    def mk_player(self, players: Set[Player]):\n",
    "        def take_turn(state: State, player: Player, prev_act: Action) -> Action:\n",
    "            self.game.render(state, player)\n",
    "            ret = input('What is your move? ')\n",
    "            while True:\n",
    "                try:\n",
    "                    ret = self.game.parse(ret)\n",
    "                except ValueError:\n",
    "                    ret = input('What is your move? ')\n",
    "                else:\n",
    "                    return ret\n",
    "                \n",
    "        return GamePlayer(players, take_turn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "TTTState = Tuple[np.ndarray, int]\n",
    "\n",
    "class TTT(Game[TTTState]):\n",
    "    _board_shape = (9,)\n",
    "    num_actions = 9\n",
    "    num_players = 2\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def gen_root(self) -> Tuple[TTTState, Player]:\n",
    "        return np.zeros(TTT._board_shape, dtype=np.int), 1\n",
    "\n",
    "    def do_action(self, s: TTTState, a: Action, p: Player) -> Tuple[TTTState, Player]:\n",
    "        new_board = np.copy(s)\n",
    "        new_board[a] = -1 + 2 * p\n",
    "        return (new_board, 1 - p)\n",
    "\n",
    "    def valid_actions(self, board: TTTState, _: Player) -> np.ndarray:\n",
    "        return board == 0\n",
    "\n",
    "    _end_idx = sum([[3 * i, 3 * i + 1, 3 * i + 2, i, i + 3, i + 6 ] for i in range(3)], [])\n",
    "    _end_idx = np.array(_end_idx + [2, 4, 6, 0, 4, 8])\n",
    "    _inv_end_idx = np.array(sum([[3 * i, i] for i in range(3)], []) + [2, 0])\n",
    "    def check_end(self, board: TTTState, player: Player) -> Player:\n",
    "        ret = np.zeros(2, dtype=np.bool)\n",
    "        z = np.flatnonzero(np.abs(np.sum(board[TTT._end_idx].reshape(8, 3), axis=1)) == 3)\n",
    "        if z.size:\n",
    "            ret[(board[TTT._inv_end_idx[z[0]]] + 1) // 2] = True\n",
    "        elif np.all(board):\n",
    "            ret[:] = True\n",
    "        return ret\n",
    "    \n",
    "    ch = {-1: 'o', 0: ' ', 1: 'x'}\n",
    "    def render(self, state: State, player: Player):\n",
    "        for i in range(3):\n",
    "            if i != 0:\n",
    "                print('-----')\n",
    "            for j in range(3):\n",
    "                if j != 0:\n",
    "                    print('|', end='')\n",
    "                print(TTT.ch[state[3 * i + j]], end='')\n",
    "            print('')\n",
    "        print('%s\\' turn' % ('x' if player == 1 else 'o'))\n",
    "        \n",
    "    def parse(self, s: str):\n",
    "        try:\n",
    "            x, y = s.split()\n",
    "            res = 3 * (int(x) - 1) + int(y) - 1\n",
    "        except:\n",
    "            raise ValueError\n",
    "        return res\n",
    "                \n",
    "    \n",
    "ttt = TTT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator(Generic[State]):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def eval_states(states: np.ndarray, players: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        pass\n",
    "    \n",
    "    def eval_state(state: State, player: Player) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        pass\n",
    "    \n",
    "class EvaluatorFromFn(Evaluator[State]):\n",
    "    def __init__(self, fn, plural=False):\n",
    "        if plural:\n",
    "            self.eval_states = fn\n",
    "        else:\n",
    "            self.eval_state = fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_puct = 1.0\n",
    "dirichlet_eps = 0.25\n",
    "dirichlet_alpha = 0.3\n",
    "\n",
    "class MCTSNode(Generic[State]):\n",
    "    def __init__(self, state: State):\n",
    "        self.state = state\n",
    "        \n",
    "    @property\n",
    "    def terminal(self):\n",
    "        raise NotImplemented\n",
    "\n",
    "class InternalMCTSNode(MCTSNode[State]):\n",
    "    terminal = False\n",
    "    \n",
    "    def __init__(self, state: State, probs: np.ndarray, player: Player) -> None:\n",
    "        super().__init__(state)\n",
    "        self.player = player\n",
    "        \n",
    "        self.actions = np.flatnonzero(probs)\n",
    "        self.inv_actions = np.empty_like(probs, dtype=np.int)\n",
    "        self.inv_actions[self.actions] = np.arange(self.actions.size)\n",
    "        \n",
    "        self.N: np.ndarray = np.zeros_like(self.actions, dtype=np.int)\n",
    "        self.W: np.ndarray = np.zeros_like(self.actions, dtype=np.float)\n",
    "        self.Q: np.ndarray = np.zeros_like(self.actions, dtype=np.float)\n",
    "        self.P: np.ndarray = probs[self.actions]\n",
    "        \n",
    "        self.total_visits: int = 1\n",
    "        self.children: List[MCTSNode] = [None] * probs.size\n",
    "            \n",
    "    def select_action(self) -> Action:\n",
    "        if not self.actions.size:\n",
    "            return None\n",
    "        \n",
    "        root_total = math.sqrt(self.total_visits)\n",
    "        \n",
    "        selection_priorities = self.Q + c_puct * root_total * self.P / (1 + self.N)\n",
    "        return self.actions[np.argmax(selection_priorities)]\n",
    "    \n",
    "    def backup(self, a, vs):\n",
    "        self.total_visits = self.total_visits + 1\n",
    "        idx = self.inv_actions[a]\n",
    "        self.N[idx] = self.N[idx] + 1\n",
    "        self.W[idx] = self.W[idx] + vs[self.player]\n",
    "        self.Q[idx] = self.W[idx] / self.N[idx]\n",
    "    \n",
    "    def probs(self, temp) -> np.ndarray:\n",
    "        if temp == 0.0:\n",
    "            ret = np.zeros_like(self.inv_actions)\n",
    "            ret[self.actions[np.argmax(self.N)]] = 1.0\n",
    "            return ret\n",
    "        exps = self.N ** (1.0 / temp)\n",
    "        ret = np.zeros_like(self.inv_actions, dtype=np.float)\n",
    "        ret[self.actions] = exps / np.sum(exps)\n",
    "        return ret\n",
    "    \n",
    "class LeafMCTSNode(MCTSNode[State]):\n",
    "    terminal = True\n",
    "    \n",
    "    def __init__(self, state: State, values: np.ndarray):\n",
    "        super().__init__(state)\n",
    "        self.values = values\n",
    "\n",
    "class MCTSAlgorithm(GameAlgorithm[State]):\n",
    "    def __init__(self, \n",
    "                 game: Game[State], \n",
    "                 evaluator: Evaluator[State], \n",
    "                 search_size: int,\n",
    "                 temp: float = 0.0):\n",
    "        self.game = game\n",
    "        self.evaluator = evaluator\n",
    "        self.search_size = search_size\n",
    "        self.temp = temp\n",
    "        \n",
    "    def mk_player(self, players: Set[Player]) -> GamePlayer[State]:\n",
    "        return MCTSPlayer(self, players)\n",
    "        \n",
    "    def _do_search(self, node):\n",
    "        for _ in range(self.search_size):\n",
    "            cur_node = node\n",
    "            history: List[Tuple[MCTSNode, Action]] = []\n",
    "\n",
    "            while cur_node and not cur_node.terminal:\n",
    "                action = cur_node.select_action()\n",
    "                history.append((cur_node, action))\n",
    "                cur_node = cur_node.children[action]\n",
    "\n",
    "            rewards: float\n",
    "            if cur_node:\n",
    "                rewards = cur_node.values\n",
    "            else:\n",
    "                prev_node = history[-1][0]\n",
    "                action = history[-1][1]\n",
    "                new_state, player = self.game.do_action(\n",
    "                    prev_node.state, action, prev_node.player)\n",
    "                result = self.game.check_end(new_state, player)\n",
    "                if np.any(result):\n",
    "                    rewards = rewards_from_result(result)\n",
    "                    prev_node.children[action] = LeafMCTSNode(new_state, rewards)\n",
    "                else:\n",
    "                    probs, rewards = self.evaluator.eval_state(new_state, player)\n",
    "                    prev_node.children[action] = InternalMCTSNode(new_state, probs, player)\n",
    "\n",
    "            for hnode, hact in history:\n",
    "                hnode.backup(hact, rewards)\n",
    "        \n",
    "        return node.probs(self.temp)\n",
    "        \n",
    "class MCTSPlayer(GamePlayer[State]):\n",
    "    def __init__(self,\n",
    "                 alg: MCTSAlgorithm[State],\n",
    "                 players: Set[Player]):\n",
    "        super().__init__(players)\n",
    "        self.alg = alg\n",
    "        self.node: MCTSNode[State] = None\n",
    "        self.hist: List[Tuple[State, np.ndarray, Player]] = []\n",
    "\n",
    "    def _take_turn(self, state: State, player: Player, prev_act: Action) -> Action:\n",
    "        if self.node != None:\n",
    "            self.node = self.node.children[prev_act]\n",
    "        if self.node == None:\n",
    "            probs, _ = self.alg.evaluator.eval_state(state, player)\n",
    "            if prev_act == None:\n",
    "                mask = probs != 0\n",
    "                probs[mask] = ((1 - dirichlet_eps) * probs[mask] + \n",
    "                         dirichlet_eps * np.random.dirichlet(np.full_like(probs[mask], dirichlet_alpha)))\n",
    "            self.node = InternalMCTSNode(state, probs, player)\n",
    "\n",
    "        probs = self.alg._do_search(self.node)\n",
    "        self.hist.append((state, probs, player))\n",
    "        return sample(probs)\n",
    "\n",
    "    def _watch_turn(self, state: State, player: Player, prev_act: Action):\n",
    "        if self.node == None:\n",
    "            return\n",
    "\n",
    "        self.hist.append((state, self.node.probs(self.alg.temp), player))\n",
    "        self.node = self.node.children[prev_act]\n",
    "\n",
    "def rewards_from_result(game_result: np.ndarray):\n",
    "    winners = np.count_nonzero(game_result)\n",
    "    if winners == game_result.size:\n",
    "        return np.zeros_like(game_result, dtype=np.float)\n",
    "    losers = game_result.size - winners\n",
    "    values = np.full_like(game_result, -1.0 / losers, dtype=np.float)\n",
    "    values[game_result] = 1.0 / winners\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcts_play_self(game: Game[State], evaluator: Evaluator[State], search_size: int, temp: float = 0.0):\n",
    "    end_state, result, players = play_game(game, [([0, 1], MCTSAlgorithm(game, evaluator, search_size, temp))])\n",
    "    hist = players[0].hist\n",
    "    rewards = rewards_from_result(result)\n",
    "    \n",
    "    return [(state, probs, rewards, player) for state, probs, player in hist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ttt_eval_state(s: TTTState, p: Player) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    acts = np.zeros(TTT._board_shape)\n",
    "    valid_acts = ttt.valid_actions(s, p)\n",
    "    acts[valid_acts] = 1.0 / np.count_nonzero(valid_acts)\n",
    "    return (acts, np.zeros(2))\n",
    "ttt_evaluator = EvaluatorFromFn(_ttt_eval_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.54 s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x158f8727ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x158f93c8908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x158f983a208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x158f942ef28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x158f996ce80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x158f9446278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x158f92c7ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1596e946c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x158f989f240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#%lprun -f MCTSPlayer._do_search \n",
    "res = mcts_play_self(ttt, ttt_evaluator, 10000)\n",
    "for s, _, _, _ in res:\n",
    "    plt.matshow(s.reshape(3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_search_size = 20\n",
    "\n",
    "class Model(Generic[State]):\n",
    "    def __init__(self, game: Game[State]):\n",
    "        self.game = game\n",
    "    \n",
    "    PARAMS_TRAIN = 0\n",
    "    PARAMS_BEST = 1\n",
    "    \n",
    "    def eval_states(self, \n",
    "                    states: List[State],\n",
    "                    players: List[Player],\n",
    "                    using=PARAMS_BEST) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        probs, values = unzip([self.eval_states(states[i], players[i], using) for i in range(len(players))])\n",
    "        return np.array(probs), np.array(values)\n",
    "    \n",
    "    def eval_state(self, state: State, player: Player, using=PARAMS_BEST) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        a, v = self.eval_states([state], [player], using)\n",
    "        return a[0], v[0]\n",
    "    \n",
    "    def train(self, batch_size=64, num_iters=1000):\n",
    "        pass\n",
    "    \n",
    "    def add_data(self, \n",
    "                 states: List[State], \n",
    "                 probs: List[np.ndarray],\n",
    "                 rewards: List[float],\n",
    "                 players: List[int]):\n",
    "        pass\n",
    "\n",
    "class ExpertIterationTrainer(Generic[State]):\n",
    "    def __init__(self,\n",
    "                 model: Model[State],\n",
    "                 num_iterations: int,\n",
    "                 iteration_size: int = 100,\n",
    "                 search_size: int = 100):\n",
    "        self.model = model\n",
    "        self.game = self.model.game\n",
    "        self.num_iterations = num_iterations\n",
    "        self.iteration_size = iteration_size\n",
    "        self.search_size = search_size\n",
    "              \n",
    "    def train_player(self):\n",
    "        evaluator = EvaluatorFromFn(lambda s, p: self.model.eval_state(s, p))\n",
    "        example_games = [mcts_play_self(self.game, evaluator, example_search_size)]\n",
    "        \n",
    "        for i in range(1, self.num_iterations + 1):\n",
    "            example_games.append(mcts_play_self(self.game, evaluator, example_search_size))\n",
    "            new_probs = [self.model.eval_state(res[0], res[3]) for res in example_games[-1]]\n",
    "            example_probs = [new_probs]\n",
    "            \n",
    "            states, probs, rewards, players = self.play_games()\n",
    "                \n",
    "            self.model.add_data(states, probs, rewards, players)\n",
    "            self.model.train()\n",
    "\n",
    "            print('finished step %d' % i)\n",
    "\n",
    "        example_games.append(mcts_play_self(self.game, evaluator, example_search_size))\n",
    "        new_probs = [self.model.eval_state(res[0], res[3]) for res in example_games[-1]]\n",
    "        example_probs.append(new_probs)\n",
    "\n",
    "        return example_games, EvaluatorFromFn(lambda s, p: self.model.eval_state(s, p, using=Model.PARAMS_BEST))\n",
    "    \n",
    "\n",
    "    def play_games(self) -> Tuple[List[State], \n",
    "                                  List[np.ndarray], \n",
    "                                  List[float], \n",
    "                                  List[int]]:\n",
    "        work_q = queue.Queue()\n",
    "        result_q = queue.Queue()\n",
    "        go = queue.Queue()\n",
    "        alive = [self.iteration_size]\n",
    "        in_q = [0]\n",
    "        counter_lock = threading.Lock()\n",
    "        \n",
    "        def mp_play_self():\n",
    "            my_q = queue.Queue()\n",
    "\n",
    "            def eval_state(state: State, p: Player) -> Tuple[np.ndarray, float]:\n",
    "                with counter_lock:\n",
    "                    work_q.put(((state, p), my_q))\n",
    "                    in_q[0] += 1\n",
    "                    if alive[0] == in_q[0]:\n",
    "                        go.put(True)\n",
    "                return my_q.get()\n",
    "\n",
    "            result = mcts_play_self(self.game, EvaluatorFromFn(eval_state), self.search_size, 1.0)\n",
    "            with counter_lock:\n",
    "                alive[0] -= 1\n",
    "                result_q.put(result)\n",
    "                if alive[0] == 0:\n",
    "                    go.put(False)\n",
    "                elif alive[0] == in_q[0]:\n",
    "                    go.put(True)\n",
    "        \n",
    "        for _ in range(self.iteration_size):\n",
    "            threading.Thread(target=mp_play_self).start()\n",
    "\n",
    "        while go.get():\n",
    "            with counter_lock:\n",
    "                ret_qs = []\n",
    "                states = []\n",
    "                players = []\n",
    "                for _ in range(in_q[0]):\n",
    "                    (state, player), ret_q = work_q.get_nowait()\n",
    "                    ret_qs.append(ret_q)\n",
    "                    states.append(state)\n",
    "                    players.append(player)\n",
    "                in_q[0] = 0\n",
    "                if len(states) > 0:\n",
    "                    a, v = self.model.eval_states(states, players, using=Model.PARAMS_BEST)\n",
    "                    for j, ret_q in enumerate(ret_qs):\n",
    "                        ret_q.put((a[j], v[j]))\n",
    "                        \n",
    "        positions = []\n",
    "        while not result_q.empty():\n",
    "            positions += result_q.get_nowait()\n",
    "        return unzip(positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TTTModel(Model[TTTState]):\n",
    "    game = TTT()\n",
    "    \n",
    "    l2_loss_coeff = 0.01\n",
    "    hidden_size = 100\n",
    "    search_size = 100\n",
    "\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        ttt_hw = tf.Variable(tf.truncated_normal([9, hidden_size]))\n",
    "        ttt_hb = tf.Variable(tf.zeros([hidden_size]))\n",
    "    \n",
    "        ttt_aw = tf.Variable(tf.truncated_normal([hidden_size, 9]))\n",
    "        ttt_ab = tf.Variable(tf.zeros([9]))\n",
    "\n",
    "        ttt_vw = tf.Variable(tf.truncated_normal([hidden_size, 2]))\n",
    "        ttt_vb = tf.Variable(tf.zeros([2]))\n",
    "\n",
    "        weights = [ttt_hw, ttt_hb, ttt_aw, ttt_ab, ttt_vw, ttt_vb]\n",
    "        saver = tf.train.Saver(weights)\n",
    "\n",
    "        tf_boards = tf.placeholder(tf.float32, shape=[None, 9])\n",
    "        hidden = tf.nn.relu(tf.matmul(tf_boards, ttt_hw) + ttt_hb)\n",
    "        actions = tf.matmul(hidden, ttt_aw) + ttt_ab\n",
    "        values = tf.tanh(tf.matmul(hidden, ttt_vw) + ttt_vb)\n",
    "\n",
    "        tf_probs = tf.placeholder(tf.float32, shape=[None, 9])\n",
    "        tf_zs = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "\n",
    "        loss = tf.reduce_mean(tf.square(tf_zs - values))\n",
    "        loss = loss + tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(labels=tf_probs, logits=actions))\n",
    "        for w in weights:\n",
    "            loss = loss + l2_loss_coeff * tf.nn.l2_loss(w)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "        \n",
    "    chkpnt_folder = os.path.join('.', 'checkpoints')\n",
    "    if not os.path.exists(chkpnt_folder):\n",
    "        os.makedirs(chkpnt_folder)\n",
    "    chkpnt_file = os.path.join(chkpnt_folder, 'ttt') \n",
    "            \n",
    "       \n",
    "    def __init__(self):\n",
    "        super().__init__(self.game)\n",
    "        \n",
    "        self.states = np.empty((0, 9))\n",
    "        self.probs = np.empty((0, 9))\n",
    "        self.rewards = np.empty((0, 2))\n",
    "        self.players = np.empty((0,), dtype=np.int)\n",
    "        self.cutoffs = []\n",
    "            \n",
    "        self.best_evaluator = EvaluatorFromFn(lambda s, p: self.eval_state(s, p, using=Model.PARAMS_BEST))\n",
    "        self.train_evaluator = EvaluatorFromFn(lambda s, p: self.eval_state(s, p, using=Model.PARAMS_TRAIN))\n",
    "        \n",
    "        self.train_step = 0\n",
    "        self.train_sess = None\n",
    "        self.best_sess = None\n",
    "        self.best_chkpnt = None\n",
    "        \n",
    "    def __enter__(self):\n",
    "        self.train_sess = tf.Session(graph=self.graph)\n",
    "        self.best_sess = tf.Session(graph=self.graph)\n",
    "        self.train_sess.__enter__()\n",
    "        self.best_sess.__enter__()\n",
    "        \n",
    "        self.train_sess.run(tf.global_variables_initializer())\n",
    "        self.best_chkpnt = self.saver.save(self.train_sess, self.chkpnt_file, 0)\n",
    "        self.saver.restore(self.best_sess, self.best_chkpnt)\n",
    "  \n",
    "        return self\n",
    "        \n",
    "    def __exit__(self, tp, val, traceback):\n",
    "        self.best_sess.__exit__(tp, val, traceback)\n",
    "        self.train_sess.__exit__(tp, val, traceback)\n",
    "        \n",
    "    def eval_states(self, \n",
    "                    states: List[TTTState],\n",
    "                    players: List[Player],\n",
    "                    using=Model.PARAMS_BEST) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        feed = { self.tf_boards: np.array(states) }\n",
    "        sess = self.best_sess if using == Model.PARAMS_BEST else self.train_sess\n",
    "        actions, values = sess.run([self.actions, self.values], feed_dict=feed)\n",
    "        for i in range(actions.shape[0]):\n",
    "            action = actions[i]\n",
    "            poss_actions = self.game.valid_actions(states[i], players[i])\n",
    "            action[~poss_actions] = 0.0\n",
    "            action[poss_actions] = softmax(action[poss_actions])\n",
    "\n",
    "        return (actions, values)\n",
    "    \n",
    "    def eval_state(self, state: State, player: Player, using=Model.PARAMS_BEST) -> Tuple[np.ndarray, float]:\n",
    "        a, v = self.eval_states([state], [player], using)\n",
    "        return a[0], v[0]\n",
    "    \n",
    "    def train(self, batch_size=64, num_iters=1000):\n",
    "        self.train_step += 1\n",
    "        \n",
    "        for i in range(1000):\n",
    "            select = np.random.choice(np.arange(len(self.states)), 64)\n",
    "\n",
    "            feed = {\n",
    "                self.tf_boards: self.states[select],\n",
    "                self.tf_probs: self.probs[select],\n",
    "                self.tf_zs: self.rewards[select]\n",
    "            }\n",
    "            \n",
    "            self.train_sess.run(self.optimizer, feed_dict=feed)\n",
    "                \n",
    "        if self.train_is_better():\n",
    "            self.best_chkpnt = self.saver.save(self.train_sess, self.chkpnt_file, self.train_step)\n",
    "            self.saver.restore(self.best_sess, self.best_chkpnt)\n",
    "        else:\n",
    "            self.saver.restore(self.train_sess, self.best_chkpnt)\n",
    "\n",
    "    def train_is_better(self):\n",
    "        tot_games = 10\n",
    "        reward = 0\n",
    "        best_alg = MCTSAlgorithm(self.game, self.best_evaluator, self.search_size)\n",
    "        train_alg = MCTSAlgorithm(self.game, self.train_evaluator, self.search_size)\n",
    "        for i in range(tot_games):\n",
    "            reward += rewards_from_result(play_game(self.game, [({0}, best_alg), ({1}, train_alg)])[1])[1]\n",
    "            reward += rewards_from_result(play_game(self.game, [({0}, train_alg), ({1}, best_alg)])[1])[0]\n",
    "        \n",
    "        avg_reward = reward / (2 * tot_games)\n",
    "        print(avg_reward)\n",
    "        return avg_reward > 0.1\n",
    "            \n",
    "    \n",
    "    def add_data(self, \n",
    "                 states: List[TTTState], \n",
    "                 probs: List[np.ndarray],\n",
    "                 rewards: List[float],\n",
    "                 players: List[int]):\n",
    "        start_i = 0\n",
    "        if len(self.cutoffs) > 3:\n",
    "            start_i = self.cutoffs[0]\n",
    "            self.cutoffs = self.cutoffs[1:]\n",
    "        self.states = np.concatenate([self.states[start_i:], states])\n",
    "        self.probs = np.concatenate([self.probs[start_i:], probs])\n",
    "        self.rewards = np.concatenate([self.rewards[start_i:], rewards])\n",
    "        self.players = np.concatenate([self.players[start_i:], players])\n",
    "        self.cutoffs.append(len(players))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from .\\checkpoints\\ttt-0\n",
      "-0.5\n",
      "INFO:tensorflow:Restoring parameters from .\\checkpoints\\ttt-0\n",
      "finished step 1\n",
      "0.35\n",
      "INFO:tensorflow:Restoring parameters from .\\checkpoints\\ttt-2\n",
      "finished step 2\n",
      "-1.0\n",
      "INFO:tensorflow:Restoring parameters from .\\checkpoints\\ttt-2\n",
      "finished step 3\n",
      "-0.85\n",
      "INFO:tensorflow:Restoring parameters from .\\checkpoints\\ttt-2\n",
      "finished step 4\n",
      "0.0\n",
      "INFO:tensorflow:Restoring parameters from .\\checkpoints\\ttt-2\n",
      "finished step 5\n",
      "play against computer? (y/n)y\n",
      " | | \n",
      "-----\n",
      " | | \n",
      "-----\n",
      " | | \n",
      "x' turn\n",
      "What is your move? 2 2\n",
      " | | \n",
      "-----\n",
      " |x| \n",
      "-----\n",
      " |o| \n",
      "x' turn\n",
      "What is your move? 3 1\n",
      " | | \n",
      "-----\n",
      " |x|o\n",
      "-----\n",
      "x|o| \n",
      "x' turn\n",
      "What is your move? 1 3\n",
      "play against computer? (y/n)y\n",
      " | | \n",
      "-----\n",
      " | | \n",
      "-----\n",
      " | | \n",
      "x' turn\n"
     ]
    }
   ],
   "source": [
    "with TTTModel() as model:\n",
    "    trainer = ExpertIterationTrainer(model, 5)\n",
    "    #%lprun -f TTTModel.train \n",
    "    examples, evaluator = trainer.train_player()\n",
    "    while input('play against computer? (y/n)') == 'y':\n",
    "        play_game(ttt, [({0}, MCTSAlgorithm(ttt,  evaluator, 10)), ({1}, UserAlgorithm(ttt))])\n",
    "    print(evaluator.eval_state(*ttt.gen_root()))\n",
    "\n",
    "for g in examples[::1]:\n",
    "    plt.matshow(g[-1][0].reshape(3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
